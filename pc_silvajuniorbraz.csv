Mechanics|Description|Score
Abstraction| We see abstraction being treated mainly as a thought process, usually carrying information from a context to another, producing a contrast (particular-general, concrete-abstract, complex-simple). As reinforced by its etymology, it brings the idea of drawing, borrowing, capturing something. But not a real object, capturing information, a set of features. In the CT literature, this is directed towards capturing the essentials, leaving behind unnecessary details and, on other approaches, capturing common characteristics. Beyond capturing features, we see that sometimes this process keep a connection with the original object, other times it is completely discarded, so we will make a distinction coining two terms: shallow abstraction, that is just a representation, a short/simple/easy way to refer to something longer/more complex whose details are not needed in the given moment; and deep abstraction, that no longer represents something, instead, it captures only a few features of it, becoming something by its own, simpler than the origin. The abstraction line is illustrated on Figure 3, showing the concepts involved using the examples of dices, cubes and polygons. Abstraction may be used merely to represent something in a simpler way (shallow abstraction); or to reach a concept by its own (deep abstraction), extracting a subset of features from a more complex, concrete object. Abstractions can extract features from other abstractions, generating layers of abstraction, where the uppers contain less details, with simple, abstract objects, while the lowers contain more details, with complex, concrete objects. Because of this feature extraction characteristic, abstraction relates to pattern recognition, that can be seen as finding the same features (e.g. showing a number on each side) on multiple objects. What also relates to generalization, represented by red arrows, naming the abstraction made of the features extracted from the multiple objects the arrow pass through. |0
Decomposition|Our vision of this concept is that when we treat decomposition we are also treating a bunch of other concepts and phenomena around the process itself, of breaking the problem down. Everything involved follows a “jigsaw puzzle thinking”, it is all about parts and wholes: how a full picture can be fragmented into pieces (decomposition); how to assemble them all into one (composition); how to put a new piece into the current puzzle (integration); how their shapes fit each other (interface); what each piece form with the adjacent ones (interrelationship); what can be seen in the full picture but not in each piece (emergence); and which pieces may be repeated over other regions of the puzzle (reuse).The decomposition line is illustrated on Figure 4, showing the concepts involved using the examples of jigsaw puzzle pieces and polygons. The contrast between wholes and parts is highlighted in this line. Decomposition breaks down wholes into parts. Composition assemble parts into wholes. New parts may be integrated into existing wholes. A part that is responsible for interacting with the external environment is an interface. Parts establish several relations with each other within the system, which should be accounted when decomposing/composing. The reuse of parts usually makes the decomposition worth. Decomposition may lead to recursion when the parts are similar in nature to the whole. While composition may lead to emergence, when the whole has a feature/behavior none of the parts have.|0
Algorithm|Algorithms are commonly approached highlighting series of steps (ISTE and CSTA, 2011; Selby and Woollard, 2014; Berland and Lee, 2011; Csizmadia, Curzon, Dorling, Humphreys, Ng, Selby and Woollard, 2015), and flow of control (Lu and Fletcher, 2009; Grover and Pea, 2013; Shute, Sun and Asbell-Clarke, 2017). In addition to those two concepts, we see something very particular to algorithms: they must be well-defined, unambiguous and finite, so that a machine could run it. We think the close relationship with the underlying computational model is best left for the word “program”, which we are going to define as an algorithm so precisely defined that each of its steps are either a collection of other steps or a trivial instruction that the given computational model is able to correctly perform. The computational models that shaped how we define algorithms in CS evolved from sequential monolithic machines to powerful networks of several multicore processors, what brings parallelism to the discussion. The algorithm line is illustrated on Figure 5, showing the concepts involved using a flowchart of a parallel algorithm as example. Algorithms are made of a finite collection of unambiguous steps organized by flow of control structures, such as sequences, loops and conditionals. It might be subdivided into procedures that refer to specific actions. When considering resources/data, algorithms may receive them as inputs and/or return them as outputs. When parallelism comes into play, algorithms may have different flows of execution, each with its own haste, what often leads to the necessity of synchronization. Parallelism also raises additional concerns for resources/data, as the different flows might share memories (leading to dispute for resources), or have exclusive access (potentially leading to deadlocks). For the execution of an algorithm, it is necessary to consider what will execute it, the underlying computational model.|0
Data|If we consider “datum” a unit of information, then data is information represented in a discrete manner, since it would be a collection of units of information. That connects the fact of being a plural to newer definitions as “transmittable and storable information by which computer operations are performed” (Etymonline, 2020), since our computer machines operate based on a discrete system: binary numbers. Data is essentially encoded by structures to define how its units and their combinations are mapped into respective meanings and how they can be managed or transformed. Data structures commonly refer to stacks, graphs, charts, tables and classes. But following the aforementioned description, the binary number system we just talked about are data structures too. So, we are classifying data structures as semiotic, primitive or higher-order. Then we deal with data making use of the data practices mentioned in the CT literature (ISTE and CSTA, 2011; Wing, 2011; Shute et al., 2017), onto which we add (data) interpretation. The data line is illustrated on Figure 6, showing the concepts involved using a text, binary code, sign language, Boolean, string, float, table, chart and graph as examples of data structures. Data is information made up by units. So, information must first be gathered/collected, then represented through data structures, to then be transformed, visualized, interpreted and analyzed. Data structures organize data to give them meaning, they can define: how to communicate/express (semiotic); the values we work with (primitive); or how those values are arranged (higher-order).|0
Automation|Our vision is that what is important about automation for problem solving is its characteristic of making processes automatic, not the computers (or whatever) that will execute them automatically. An important discussion is that when we automate something, we are not delivering it free will. We are, at some point, deciding what it will do. But once something is automated, it does not ask for directions, it automatically does something based on its own rules. The tricky part is that we are the ones who defined its rules. We grant them a certain autonomy, but only after defining their rules. Ultimately, the autonomy granted is so great it could redefine its own rules, something we could expect from a strong artificial intelligence. For instance, our smartphones or desktops have several layers of autonomy, they do not ask us which process should go in which core or which video encoding strategy it should use to record. In fact, sometimes they do not even ask us if we want to update their operating system. Automation freed them and us from the torment of being constantly annoyed by questions about what to do. As many CS artifacts are tools, usually presenting a high degree of autonomy and the creation of those are referred in the CT literature, we are including their processes of creation in this line. The automation line is illustrated on Figure 7, showing the concepts involved using an effort diagram, since automation revolves around labor saving. Automation occurs when something becomes automatic to you, independent of your guidance. It happens by ordering others to do something for you, but if that is not trivial, the others will need to constantly consult you about what to do. A better solution is to offer them instructions, so they can consult the instructions instead of you. Instructing machines lead us to the creation of artifacts (software development and hardware tinkering), enables scaling projects and powers simulations.|0
Evaluation| From the word history we could see evaluation as the product of the analysis, but we don’t see much use on this differentiation, and since the words are already being used interchangeably, we will keep treating them as synonyms. We see optimization as the process of granting efficiency, which is a property. Other properties, such as correctness, validity and safety are mentioned along evaluation/analysis tasks in the CT literature, but as efficiency is by far the most considered property we are highlighting it individually. Furthermore, we see testing and debugging as evaluation strategies. And systematic reorganizations such as classification systems, clustering strategies and sorting algorithms are based on detailed examinations of properties, so we are including it in this line too. The evaluation line is illustrated on Figure 8, showing the concepts involved. Evaluation is done through a detailed examination, an analysis. It might judge a property, such as efficiency and correctness, or lead to building up predictions. A better evaluation can be assured through tests, by experimenting common and critical situations, or through tracking and debugging, by spotting and fixing errors. The examination also may lead to reorganization tasks, such as classification, clustering and sorting.|0