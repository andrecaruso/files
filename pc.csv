Mechanics|Description|Score
Data|The word “data” dates back to 1640s, being used as the plural of “datum”, from Latin datum “given”, neuter past participle of dare “to give” (ONLINE ETYMOLOGY DICTIONARY, 2020). The meaning evolved through the years, from “a fact given as the basis for calculation in mathematical problems” and “numerical facts collected for future reference” to “transmittable and storable information by which computer operations are performed” (ONLINE ETYMOLOGY DICTIONARY, 2020). Nowadays, data can be found with the following meanings in dictionaries: “factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation”; “information in digital form that can be transmitted or processed”; and “information output by a sensing device or organ that includes both useful and irrelevant or redundant information and must be processed to be meaningful”. Broadest of them all. Some of the aforementioned definitions are bound to computer machines, but we believe there is a bunch of data that was used even before the machines of the Digital Era, such as all the data manually collected by census takers for inquires. Just like we did with our definition of Computer, we are stretching it beyond machines. An interesting point to note, is that many of those definitions explicitly tell or at least imply that data is transmittable, which lead us back to Explicit Knowledge. But then, we are confronted with the difference of knowledge to information. To avoid the philosophical trap of an endless loop of definitions, their origins and meanings, we are are not digging into the definitions of knowledge and information, just assume the common sense for those words, that actually could apply to nearly anything. Quantized. The word “datum” is already used, specially in cartography, as a reference point from which scientific measurements are made, which comes from its Latin origin “given”, i.e. an accepted fact (INTRODUCTION TO DATUM, 2020). Despite of this, we will be facing datum as the singular of data, which our aforementioned definitions suggest that should be transmittable. Due to our real world limitations, we can only transmit finite, discrete information, even though we may represent infinity, continuums and analogical information by assuming strategies such as inductive reasoning or a massively large amount of possibilities to sample the whole, e.g. floating points. Therefore, data is a quantized information, i.e. information composed by a finite set of discrete units. Term 15 – Data: A quantized information. Information made up by a finite set of discrete units, what allows it to be transmitted and operated by Computers. Guiding the usage. Data is essentially encoded by structures to define how its units and their combinations are mapped into respective meanings and how they can be managed or transformed. Data structures commonly refer to stacks, graphs, charts, tables and classes. But following the aforementioned definition, the binary number system that underlies most of our contemporary Computer machines are data structures too. So, we are classifying data structures as semiotic, primitive or higher-order. Semiotic data structures are at the very bottom, they are used to define how to communicate or express, such as numeral systems, codes and natural, sign, formal and programming languages. Those thing may usually do not be considered a data structure, but they fit our definition of Data Structure, so we are including them. Primitive data structures can be commonly found with the name of data types, such as Booleans, characters and floating points, they are used to define the values we want to work with. Higher-order data structures are what people usually refer to when talking about data structures, such as lists, tables, charts and graphs, they are used to organize values (which, in turn, are defined by the primitive data structures). Term 16 – Data Structure: A model that defines how the building blocks of Data are organized in order to raise up meaning, how they can relate and how to access, modify or operate upon the Data modeled by it. CT data practices. In the CT literature data often goes along with gathering/collection, transformation, analysis and representation (ISTE; CSTA, 2011; WING, 2011; BARR; STEPHENSON, 2011; SHUTE; SUN; ASBELL-CLARKE, 2017). Data gathering and data collection seems to be used interchangeably, so we are going to keep them as synonyms. Data collection/gathering is the prior process of obtaining the information to generate Data. Data representation is the process of defining or choosing the Data Structure and modeling the Data accordingly. Data transformation is the process of migrating Data from a Data Structure to another. We are also adding Data interpretation, the process of recovering the information used to generate the Data. That is, reaching the meaning of the values in a given Data Structure, ideally the intended meanings. Analysis. In a cross with evaluation, Data may be systematically analyzed in a way unrestricted information may not. For instance, data mining can find patterns by organizing data entries into clusters only if the information is made of comparable pieces. Seeing information into a given Data Structure may reveal or highlight features or properties of the information, such as proportion notions on a pie chart or trends in a line chart. Well-defined data structures enables formal proofs, potent mathematical and logical tools for understanding and predicting phenomena. Therefore, Data analysis is the process of building new information about the existing Data, based on them. That is, analyzing data is about drawing conclusions, classifying and spotting properties harnessing the Data Structure used to represent the information. Visualization. With the rise of the big data field and the massive amounts of data being produced and shared every second nowadays a special concern came to light: how to best visualize all this data. This visualization effort targets not only analysis, but communication. How to inform, how to show, how to pass on information? More specifically, in which form? Thus, the concerns with the Data Structure extrapolates intern operation and management to external relations, to present it to third parties. Thus, Data visualization is the process of accessing and/or granting access to Data, which relies on representation and enables/powers interpretation and analysis. Data in CS. We can’t say CS created data, but for sure CS warmly adopted and embraced it. Maps and charts are hard to be hand drawn if we are willing to correctly have them in scale. Computer machines have drastically contributed to their plotting, revolutionizing data visualization regardless of the field or context. Those precise representations were already far, if not unreachable from human hands, but as CS advances we use and generate more and more data. It has gone to the point that humans couldn’t handle them without machines automating a massive amount of work, in fact, a whole new field has emerged dedicated entirely to treat, analyze and visualize huge amounts of data using automatic tools and techniques: the Big Data field. Examples. Figure 10 illustrates the main concepts discussed in this subsection: data structures (semiotic, primitive and higher-order), gathering/collection, representation, transformation, visualization, interpretation and analysis. In the sequence, a frame shows examples of what is included in the CT Line of Data: (CTS) Topics such as charts and cartography (higher-order Data Structures) in geography classes and genetic code (semiotic Data Structures) in biology classes; (CTS) Expressions such as switching between data structures (transformation) and creating data models (representation); and (CTS) Sensibility(ies) such as comprehension of the disposed information (interpretation) and clustering (analysis).Topics – Data: ; (Geography) Cartography ; (Interdisciplinary) Charts ; (Math) Function Curves ; (Chemistry) Periodic Table ; (Physics) Systems of Measurement ; (Physics) Quantum Mechanics ; (Biology) Genetic Code Expressions – Data: ; Gathering Information ; Plotting ; Creating Data Models ; Switching Between Data Structures Sensibilities – Data: ; Comprehension of the Disposed Information ; Spotting Properties based on the Data Structure ; Clustering ; Decoding. |0
Algorithm|A finite collection of unambiguous steps. Relying on the computational model. AHO (2012) spends six paragraphs explaining Turing Machines to further define algorithms in terms of them, just to point out how much detail is involved in precisely defining computation, even for one of the simplest models of computation. DENNING (2017) spots how the “new CT” movement has resignified algorithms, taking away the notion of the computational models. He argues that now algorithms are depicted as recipes for carrying out tasks, something for all kinds of information processors, including humans. In contrast to what it used to be: a set of directions to control a computational model, deriving their precision from it. Programs. We think the close relationship with the underlying computational model is best left for the word “program”, which we are going to define as an Algorithm so precisely defined that each of its steps are either a collection of other steps or a trivial instruction. By trivial instruction we mean one that is given, i.e. always easily, correctly understood and/or performed. Since what is trivial depends of the model, a program is always relative to the model. A programming language is a set of trivial instructions. We as computers. Following our definition of Computer, humans are perfectly fit for being classified as one. It might be nonviable to exhaustively map the full set of computations we are able to perform, yet, it would be enough to model a subset that covers what is needed for a given task. Thereafter, we would need to map those computations into trivial instructions the human is able to perform. At last, a programming language of a program for a human could be any restricted natural language, mimics or drawings in which the human is literate in and able to unambiguously interpret. Ambiguity. The major caveat with the process above (to formally use a human as a Computer carrying out Algorithms) is the difficulty of dealing with ambiguity. Probably the most distinctive force that shapes our minds in a certain way in CS is objectivity at its finest. Instructions and data are so precisely mapped into their meanings that a mechanical, mindless being can “understand” them, that there is no other way to do so. That is exactly the special touch Algorithms have, they are a special case of a composite instruction, an unambiguous one, which pervades all CS. Flow and Parallelism. When there is order in a collection of steps, it might express different shapes. A long sequence that starts over when it is finished is a cycle, if it pops out of another structure, it is a loop, each repetition is called an iteration. A sequence that branches into two or more is a conditional if it proceeds to one of its options, or a fork if it proceeds simultaneously to more. The whole dimension of parallelism is a matter of management of the flow of the steps. It comes with major concerns about different cadences, synchronization, resource dispute and locks (paths that are blocked due to their dependency of another). Although we consider recursion a form of Decomposition where the smaller parts share the same nature of the whole, it is often approached in the context of Algorithms we use to implement it. Algorithms in CS. From all the concepts addressed by CT, this is the one that needs less to justify its presence in CS. DENNING (2017) points out that names such as “algorithmizing” and “algorithmic thinking” were already used to address how CS changes the way one think prior to CS. Computer scientists assumed the challenge to make mindless beings perform complex tasks and it shaped the way they would envision, define and order instructions. A quest to eliminate intuition, subjectivity and ambiguity so those that have none of that (machines) could follow our orders. Additionally, multiple cores and huge distributed systems granted a place for concurrency on our instructions. our instructions. Examples. Figure 9 illustrates the main concepts discussed in this subsection: unambiguous steps, flow of control (including sequence, conditionals, cycle, loop and recursive processes), parallelism (including fork, join, synchronization, cadence, deadlock, resource dispute and coroutine), subroutine and iteration. In the sequence, a frame shows examples of what is included in the CT Line of Algorithm: (CTS) Topics such as mathematical induction (unambiguous steps and iteration) in math classes, interest (iteration and eventually recursive processes on compound interest) in economics classes and life, nitrogen and hydrologic cycles in biology and chemistry classes; (CTS) Expressions such as diagramming (flow of control) and multitasking (parallelism); and (CTS) Sensibility(ies) such as awareness of dependencies (resource dispute and deadlocks). Topics – Algorithm: ; (Computer Science) Flow of Control ; (Math) Mathematical Induction ; (Math) Polynomials Solving Process ; (Economics) Interest – Simple and Compound ; (Biology) Life Cycle ; (Biology/Chemistry) Nitrogen Cycle ; (Chemistry) Hydrologic Cycle ; (Physics) Multistaged Problems Expressions – Algorithm: ; Diagramming ; Multitasking Sensibilities – Algorithm: ; Awareness of Dependencies ; Recognizing Cycles.|0
Decomposition|When we treat decomposition we are also treating a bunch of other concepts and phenomena around the process itself, of breaking the problem down; Everything involved follows a “jigsaw puzzle thinking”, it is all about parts and wholes; The contrast between wholes and parts is highlighted in this line. Decomposition may lead to recursion when the parts are similar in nature to the whole. Decomposition: how a full picture can be fragmented into pieces / how to breaks down wholes into parts; Composition: how to assemble them all into one; Composition assemble parts into wholes; Integration: how to put a new piece into the current puzzle; New parts may be integrated into existing wholes; Interface: how their shapes fit each other; A part that is responsible for interacting with the external environment is an interface. Interrelationship: what each piece form with the adjacent ones;  Parts establish several relations with each other within the system, which should be accounted when decomposing/composing; Emergence: what can be seen in the full picture but not in each piece; composition may lead to emergence, when the whole has a feature/behavior none of the parts have. Reuse: which pieces may be repeated over other regions of the puzzle; The reuse of parts usually makes the decomposition worth.Topics: (Languages) Scansion and poem structural analysis; (Math) Factoring (Computer Science) API; (Chemistry) Separation processes; (Biology) Animal and plant anatomy; (Biology) Interspecific interactions;  (Biology) Ecology. Expressions: Assembling; Reusing; Interface design; Emergence design. Sensibilities: Recognizing units as interfaces of greater systems; Spotting interrelationships; Awareness of emergent features; Reusability.|0
Automation|The automation line Go to work, machine. CT literature addresses automation as making computers (referring to our modern machines) do the work for us (BOCCONI et al., 2016; LEE et al., 2011) or using computers as tools (BARR; STEPHENSON, 2011; YADAV et al., 2014). Sometimes, they are more specific, spotting that the work left for the computers are repetitive or tedious tasks (ISTE; CSTA, 2011). We agree those are good examples of the automation process, but we are going to define automation in a broader way. Term 17 – Automation: The process of granting autonomy. Decreasing dependency of your guidance. Autonomy. One might feel weird about this term, since when we automate something, we are not delivering it free will, it is not going to decide what to do by itself, it will follow our rules. Think that when something is autonomous it does not ask for directions, it automatically does something based on its own rules. The tricky part is that we are the ones who define which rules are those to be followed by the automated being. We define their rules, and then grant them autonomy. Ultimately, the autonomy granted is so great it can redefine its own rules, something we could expect from a strong AI. It is very likely that you will hear from computer scientists or programmers that computer machines are incredibly dumb and we must tell them every single thing they must do, precisely and exactly. We correct this statement: someone must have told them at some point in time, not us, as the final users. Our smartphones or desktops have several layers of autonomy, they do not ask us which process should go in which core or which video encoding strategy it should use to record. In fact, sometimes they do not even ask us if we want to update them. Automation freed them and us from the torment of being constantly annoyed by questions about what to do. Automatic life. We can bring Automation to living beings, even to ourselves, internally. Think of a new intern that every time he/she receives a new spreadsheet, he/she asks you what to do with it. Teaching him how to classify the spreadsheets and send them to their respective archives turn this process automatic to you. New spreadsheets are going to the right archives without the need of any further action from you. Internally, anything that starts to be made unconsciously was automated, like riding a bike, your legs move accordingly without the need of thinking or planning their exact movements. Let us say that an order is about what you want to be done and an instruction is about how you want to get it done, then Automation is about instructing, not ordering. Aside from simple mechanical devices, instructions are the key of Automation, that being said, Algorithms are close friends. Remember the most special feature of Algorithms, non-ambiguity, if you pass ambiguous instructions, you may have undesired automated solutions. Use of technology. SELBY; WOOLLARD (2014) defines automation as “an implementation of abstractions by computing devices”. For MANNILA et al. (2014), it is “recognizing ways in which technology can help us accomplish tasks that would otherwise be too repetitive, unfeasible or difficult”. LEE et al. (2011) says “Automation is a labor saving process in which a computer is instructed to execute a set of repetitive tasks quickly and efficiently compared to the processing power of a human”. Those definitions look back to the first face of (General) Computational Thinking, the use of CS products. We highlight that computer machines do benefit the most from Automation so far. It is completely out of scale how much machines outperform humans in repetitive well-defined tasks. But the key to benefit from this high performance is by letting them running with as less of dependence of our interventions as possible. Tool makers. Some papers directly mention (VOOGT et al., 2015; BOCCONI et al., 2016; LEE et al., 2011), and in many others it can be inferred, that the creation of artifacts, software (as games and apps) and hardware (as robots and small electronic projects), is part of CT. If instead of creating artifacts in general, we talk about creating tools, which help us creating things, then we may fit those abilities in this CT line. Through the lens of Automation, the more automatic a tool is, the better. Beyond tools, other artifacts that significantly involve Automation are any self-contained ones, such as games, systems and simulations. Given that creating software applications are generally a fairly different process with respect to creating hardware, we are naming those processes developing and tinkering, respectively. Simulation. It can be seen as a huge crossing of all CT major lines, but we are highlighting simulation here because its core idea is letting it run on its own, to further analyze it and then relate to the real thing. Simulations enable us to predict natural phenomena by imitating it in a controlled context where we usually can isolate variables, set different conditions, the time rate, and all those things that we do not stand much control over in the real world. It is noteworthy that using simulations may be a lot closer to data visualization, while designing simulations are closer to automation. Automation in CS. The understanding of the huge gap between human and machine performances in certain tasks is the major distinction of Automation in CS. The fast scripts routinely used by CS professionals to search or create pattern-based simple files reveals this awareness. Knowing that millions and millions of a calculation that would take us years, if ever, to complete is done in a matter of seconds by the machine. But also knowing that no matter how much time you give to it, the machine is not going to conceptualize a good scene for a fantasy illustration. Unless a new deep neural network solution get designed and successfully claim those feats, then we will be entering in a new era of automation. Examples. Figure 11 illustrates the main concepts discussed in this subsection: order, instruction, development, tinkering, automated effort and simulation. In the sequence, a frame shows examples of what is included in the CT Line of Data: (CTS) Topics such as volcanism simulations in geography classes and cooperative activities/games (potentially order/instruction) in physical education classes; (CTS) Expressions such as building robots (tinkering) and creating virtual games (development and potentially simulation); and (CTS) Sensibility(ies) such as realizing autonomous alternatives. Topics – Automation: ; (Geography) Volcanism Simulations ; (Biology) Cultivation ; (Physical Education) Cooperative Activities/Games Expressions – Automation: ; Building Robots ; Creating Virtual Games ; Providing Instructions ; Replacing own effort with tools Sensibilities – Automation: ; Realizing autonomous alternatives ; Spotting tool usage opportunities.|0
Evaluation|The evaluation line Evaluation vs Analysis. “Analysis” and word derivations was found in 12 of the 28 CT defining resources reviewed, “data analysis” in four, and “evaluation” and derivations in 7 and any of those in 14. Those terms seem to be used interchangeably even though SELBY; WOOLLARD (2014) recommends to use “evaluation” but not “analysis”. MERRIAM-WEBSTER (2020) brings the following definitions for evaluation: “determination of the value, nature, character, or quality of something or someone” and analysis: “a detailed examination of anything complex in order to understand its nature or to determine its essential features: a thorough study”. We are going to use evaluation, but we consider the interchangeable use of both words acceptable. Term 18 – Evaluation: The process of defining or recognizing values through a detailed examination. Efficiency. Concepts of optimization and efficiency are even more consensual (17 out of 28) than the aforementioned broader terms. We consider efficiency a property and, in turn, property checking an evaluation task. Our review suggests that efficiency is an extraordinary property for the CT, a lot more mentioned than others or general property evaluation, so we keep highlighting it here. Efficiency is about a costbenefit analysis, while optimization is about increasing efficiency. Actually, this is how the common-sense defines optimization, because theoretically optimization is about reaching specifically the most favorable solution, design, system or state, meaning that there cannot possibly be an alternative that outmatches the optimal one. Properties. Referring to traits, qualities or virtues of something, properties may be found and/or quantified through analysis. CS turn it into an objective, mathematical process using formal verification for proving correctness, that is, proving (or disproving) that something is correct with respect of a given formal specification. Model checking, for instance, mathematically models something as a state system and presents a systematically exhaustive exploration of the states to prove specifications generally given in temporal logic. Along correctness, several other properties are commonly analyzed in CS, such as usability, portability, interoperability, flexibility, durability, reliability, safety and maintainability. Test and Debug. Also more popular in the literature than the broader terms (14 out of 28) testing and debugging are other evaluation tasks. With a common, consensual understanding, tests are experimental executions directed towards checking if or how something happens, often in a partial, controlled environment. On the other hand, what “debugging” means may vary from person to person. Some will narrow it to fixing errors (bugs) while others will rely on the systematic search for the errors that precedes their fixing. Ability-wise, we understand that debugging is a system awareness, it is about knowing what and how something is happening, internally. By having a clear vision of the internal process of something one may spot unnoticed misbehaviors, their cause and how to fix them. Prediction. One of the most useful consequences of a detailed examination is the possibility of making predictions of future events based on the subject’s behavior, features and/or properties. It is at the core of all science to hypothesize, to test and then report the findings. That early part about stating what could or should happen in a given situation is grounded on evaluation, it precedes testing, and in fact sometimes testing isn’t even necessary to validate it. Most of the theoretical development is not made on the classic empirical positivist science way, but we are skipping this debate, we just mentioned it to highlight the power of prediction. Classification. Determining efficiency, properties, if it is working or not and what should happen in the future are all external processes built on top of what we are evaluating. But analyzing its internal structures may lead us to reorganize them, classifying it, sorting it, clustering it. Classification is a matter of systematic organization, ensuring things that belong to the same class share a minimal set of features. It usually cross Abstraction because generalization and pattern recognition both ground it, and Data because visualization is its enabler and power house. Evaluation in CS. We are getting in touch with completely new ways of evaluating, AI has brought surprise even to their creators by solving problems in ways we could not anticipate (LEHMAN; CLUNE; MISEVIC, 2018). For instance, it is scary of how perfect a deep fake video can get, but if it takes a neural network to make one, why not use a neural network to detect one (GÜERA; DELP, 2018)? You know, modern problems require modern solutions, CS has been automating evaluation for a long time now, we are probably near a next step. Recalling that automation, CS spread an humongous amount of tools to improve the gathering, storing, sharing and processing of information, which increased accuracy and precision of the evaluation process of researchers across nearly all fields. Examples. Figure 12 illustrates the main concepts discussed in this subsection: detailed examination, efficiency and property check, test and debug, prediction and classification. In the sequence, a frame shows examples of what is included in the CT Line of Data: (CTS) Topics such as text analysis (detailed examination) in languages classes and experiments in general (test) in chemistry and physics classes; (CTS) Expressions such as sorting (classification) and tracking errors (debug); and (CTS) Sensibility(ies) such as taking account of properties and realizing better alternatives (efficiency).Evaluation: ; (Languages) Text Analysis ; (Languages) Grammatical Classes ; (Math) Systems of Equations ; (Chemistry) Classifications of Acids and Bases ; (Chemistry and Physics) Experiments in General Expressions – Evaluation: ; Testing ; Fixing errors ; Tracking errors ; Sorting Sensibilities – Evaluation: ; Awareness of the Internal Behavior ; Spotting Malfunction and Misbehavior ; Taking Account of Properties ; Realizing Better Alternatives|0
Abstraction|The abstraction line Grammatically Multiclass. Abstraction is the nearest-to-unanimity term about CT, yet, we have found this word being interpreted in seemingly different ways in the literature. Ironically, definitions of abstraction are often too abstract. So, we are going to investigate and define this word, which is such a versatile one. It is commonly used as two nouns (abstraction and abstract), a verb (to abstract) and an adjective (abstract), not to mention all less used derivations. We are going to further discuss and illustrate the process of abstraction to reach a definition that matches the common uses of this word in the literature. Generalizing and simplifying. The most common definitions for abstraction in the literature revolve around generalization (YADAV et al., 2014; LEE et al., 2011) and/or ignoring/hiding details (LU; FLETCHER, 2009; ANGELI et al., 2016). We elected an excerpt to summarize it: “Abstraction is “the process of generalizing from specific instances.” In problem solving, abstraction may take the form of stripping down a problem to what is believed to be its bare essentials. Abstraction is also commonly defined as the capturing of common characteristics or actions into one set that can be used to represent all other instances.” (LEE et al., 2011) An ancient concept. SENGUPTA et al. (2013) traces the history of abstraction study back to ancient Greece with Plato (360 BCE) and Aristotle (384 BCE) and their concepts of forms (qualities) and sensibles (what is perceived through sensations). Then he brings it to the later philosophers Locke and Jean Piaget, highlighting the famous abstract-concrete and particular-general distinctions: “Locke proposed two types of ideas: particular and general. Particular ideas are constrained to specific contexts in space and time. General ideas are free from such restraints and thus can be applied to many different situations. In Locke’s view, abstraction is the process in which “ideas taken from particular beings become general representatives of all of the same kind” (Locke 1690/1979).” (SENGUPTA et al., 2013) A matter of abduction. Investigating its etymological origins we may illustrate the process of abstraction as something that encompasses all the aforementioned features that are attributed to this term. But we will make a distinction here, we are coining two terms: shallow abstraction and deep abstraction, both about carrying away (abducting) some features from a context to keep them isolated in another. “We trace the origins of abstract to the combination of the Latin roots ab-, a prefix meaning “from” or “away”, with the verb trahere, meaning “to pull” or “to draw”. The result was the Latin verb abstrahere, which meant “to remove forcibly” or “to drag away”. Its past participle abstractus had the meanings “removed”, “secluded”, “incorporeal”, and ultimately, “summarized”, meanings which came to English from Medieval Latin.” (MERRIAM-WEBSTER, 2020) Shallow abstraction. LU; FLETCHER (2009) explains the estimate-divide-average algorithm (EDA) and exemplify its use in the process of finding the square root of 60 by repeated guess and check: 2 ⇒ 16 ⇒ 9.875 ⇒ 7.975 ⇒ 7.749 ⇒ 7.746, where ⇒ denotes the EDA calculation. LU; FLETCHER (2009) explains ⇒ as an abstraction of the function λg.(g/60 + g)/2, that represents the EDA calculation. Generally, scientific papers present an abstract that is a short text giving only the most important facts or ideas the rest of the paper contains. Both are good examples of what we are calling shallow abstraction, analogue to a macro of programming language, it is just a representation, a short/simple/easy way to refer to something longer/more complex. Both goes through the process of abducting features away to a new context: in the first, the fact an input is being transformed by a process is abducted away from the formula that describes it to get into a sequence of transformations that tends to the square root of 60; and in the second, the key points of the paper (problem, methods and results) are abducted away from the whole body of the text to a concise, quick-to-read summary at the beginning of the article. Note that both keep contexts heavily tied to each other. Deep abstraction. When we break this connection, for instance, if ⇒ stop representing specifically λg.(g/60 + g)/2, but represents now any function with some set of properties. Then, we are enabling generalization by cutting the bounds with the context of origin, this is deep abstraction. So, if one extract some features from a context and put it, isolated, into a new context, it is already abstraction, but shallow. If one proceeds to reinforce the isolation, completely freeing the features from its origin, it is deep abstraction. The features being carried to a new context are the abducted ones, those left behind are the abstracted ones. Whenever we say “abstract that” it means to apply this process having as abstracted features the ones referred by “that”, i.e. to left behind all of “that”. We can define generalization in terms of abstraction as the process of finding multiple sources for the same set of abducted features. Sometimes not a process. It is common to hear that CS uses abstractions, and sometimes it is not referring to the process, but to the product of it. An abstraction (when not the process itself) is anything that is intrinsically a product of the process with the same name, i.e. a set of isolated features. Anything we find in the real world can be classified in abstractions that collect some of their features, but they always have a multitude of other features those abstractions don’t capture (or better, abduct), a multitude of features being abstracted away. For instance, consider a real dog called Bob. It has all the features dogs have, but not isolatedly, they are all intertwined with their bio and physical properties and all the individualities that define Bob. Bob is not an abstraction then, but “dog” is. From Bob we define “dog” by isolating features, as being breastfed when baby, having four legs, fur, a tail and such a potent snout. The power of abstraction. Abstraction may initially draw on the reality, but it may also lay on itself. We could define mammals from “dog”, abducting its first feature only. Here we are operating upon abstractions to make new abstractions. It allows us to leave reality behind, what comes very handy to problem solving and science, since reality is out of our control. Isolating variables to precisely investigate causes is a hard work in the real world, if we abstract those variables we may lose some information, but we can keep the control, which usually is a favorable trade-off. Abstraction also grounds imagination, abducting features from the reality to recombine them in our minds. That is how we could abduct the fur from Bob and the body plan of a salamander to conceive a furry dragon like Falkor from The Neverending Story (see Dragon in Figure 6). Concrete vs abstract. Although Falkor is a fantastic being, it does not feel abstract in the movie, it feels pretty concrete. Falkor does interact with its environment, it is a complex being with its physical properties reverberating in the fictional world it is put in. Being abstract or concrete is a matter of looking like a product of abstraction or not, respectively. The more isolated the concept is, the more it looks like a set of abducted features, thus, the more abstract it is. Antonymously, the more attached to its context, the more relations it stands within, the less it looks like a set of abducted features, and then, the more concrete it is. Think of abstract art, where shapes and colors are stripped of their visual references and meanings in the real world, claiming independence. Or an entrepreneur demanding concrete examples of how the product could help his business, he/she want to hear about its practical uses by real people, a product made of something with a cost, a rarity, a viability... not a theoretical model. Pattern Recognition. We can define pattern recognition in terms of abstraction as the process of finding a set of features that could be abducted from multiple sources. That is, to recognize a pattern can be seen as the process of finding an abstraction in common. If I can abduct the same features from multiple sources, then those sources share a pattern. After all, what is a pattern if not a recurring set of features? Pattern recognition is a concept that is usually highlighted in the literature (cited in 13 out of 28 in our SLR) and in informal resources (see Annex A). Although we can define it by means of abstraction, it may be best seen as a cross with other lines, such as evaluation and data. Abstraction in CS. CS severely uses recursion of abstractions, raising what we call layers of abstraction: we draw on physical (electronic) switches to conceive bits, an abstraction used to establish boolean logic, which we draw on to build logic gates, from which we draw instructions, and then macros, functions, programs and systems. Another distinction of the abstraction in CS is that it aims to build highly independent layers, as WING (2011) points, a software developer may ignore details of the underlying hardware, the Operating System (OS), the file system, or the network. In fact, the software is meant to work independently of which hardware, OS, file system or network will support it. In contrast, the classifications we use in biology serves no purpose if they are not directed towards the underlying layer. Why would we define a dog if not to organize/categorize the similar beings that constitute it in the layer below? Term 12 – Abstraction: The process of extracting features from a source in a given context and putting them isolated in a new context; or a intrinsically isolated set of features. The features selected to the new context are said abducted. The features left behind in the source are said abstracted. The given, original context is said the lower level. The new context is said the upper level. When the abducted features keep referring specifically to its origin, the abstraction is said to be shallow. When the abducted features lose sight of its origin or is not restrained to it, the abstraction is said to be deep. Anything that looks like a product of abstraction is said abstract, contrary to concrete, which is anything that does not seem to come out a process of abstraction. Examples. Figure 7 uses dices, cubes and polygons to illustrate the main concepts discussed in this subsection: concrete and abstract things, deep and shallow abstraction processes, abducted and abstracted features, abstractions (as nouns), layers of abstraction, generalization and patter recognition. In the sequence, a frame shows examples of what is included in the CT Line of Abstraction: (CTS) Topics such as concrete and abstract nouns in languages classes and variables (generalization) in math classes; (CTS) Expressions such as modeling (which involves deep abstraction) and representing (shallow abstraction); and (CTS) Sensibility(ies) such as awareness of layers of abstraction. Topics – Abstraction: ; (Languages) Concrete and Abstract nouns ; (Languages) Summary Writing ; (Math) Variables ; (Math) Formulas ; (Biology) Taxonomy Expressions – Abstraction: ; Classifying ; Generalizing ; Modeling ; Representing Sensibilities – Abstraction: ; Recognizing abstractions ; Awareness of lower layers ; Awareness of upper layers ; Pattern Recognition.|0