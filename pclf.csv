Mechanics|Description|Score
Abstraction|Abstraction: The most common definitions for abstraction in the literature revolve around generalization (Yadav et al., 2014; Lee et al., 2011) and/or ignoring/hiding details (Lu and Fletcher, 2009; Angeli et al., 2016). We elected an excerpt to summarize it: “Abstraction is “the process of generalizing from specific instances.” In problem solving, abstraction may take the form of stripping down a problem to what is believed to be its bare essentials. Abstraction is also commonly defined as the capturing of common characteristics or actions into one set that can be used to represent all other instances.” Lee et al. (2011). Word History Sengupta et al. (2013) traces the history of abstraction study back to ancient Greece with Plato (360 BCE) and Aristotle (384 BCE) and their concepts of forms (qualities) and sensibles (what is perceived through sensations). Then he brings it to the later philosophers Locke and Jean Piaget, highlighting the famous abstract-concrete and particular-general distinctions. Etymologically, Merriam-Webster (2020) says it comes from Latin roots ab- (“from” or “away”), plus trahere (“to pull” or “to draw”). Relation with Computer Science Recursion of abstractions are severely used in CS, raising what we call layers of abstraction: we draw on physical (electronic) switches to conceive bits, an abstraction used to establish boolean logic, which we draw on to build logic gates, from which we draw instructions, and then macros, functions, programs and systems. Another distinction of the abstraction in CS is that it aims to build highly independent layers, as Wing (2011) points, a software developer may ignore details of the underlying hardware, the operating system, the file system, or the network. Sengupta et al. (2013) traces the history of abstraction study back to ancient Greece with Plato (360 BCE) and Aristotle (384 BCE) and their concepts of forms (qualities) and sensibles (what is perceived through sensations). Then he brings it to the later philosophers Locke and Jean Piaget, highlighting the famous abstract-concrete and particular-general distinctions. Etymologically, Merriam-Webster (2020) says it comes from Latin roots ab- (“from” or “away”), plus trahere (“to pull” or “to draw”). Recursion of abstractions are severely used in CS, raising what we call layers of abstraction: we draw on physical (electronic) switches to conceive bits, an abstraction used to establish boolean logic, which we draw on to build logic gates, from which we draw instructions, and then macros, functions, programs and systems. Another distinction of the abstraction in CS is that it aims to build highly independent layers, as Wing (2011) points, a software developer may ignore details of the underlying hardware, the operating system, the file system, or the network.|0
Algorithm|Algorithm: is a well known and used word in CS due to coding, programming languages and its introductory courses. Two major concepts are addressed when talking about algorithms in the COMPUTATIONAL THINKING (CT) literature, the first is a step-wise design: “series of ordered steps” (ISTE and CSTA, 2011), “step-by-step set of instructions” (Selby and Woollard, 2014), “data “recipe” or set of instructions” (Berland and Lee, 2011) or “getting to a solution through a clear definition of the steps” (Csizmadia et al., 2015). The second is flow-wise design: “algorithmic notions such as basic flow of control” (Lu and Fletcher, 2009; Grover and Pea, 2013), “Algorithms often contain sets of related conditional logic” (Berland and Lee, 2011) or “Design logical and ordered instructions” (Shute et al., 2017). Denning (2017) spots how the “new COMPUTATIONAL THINKING (CT) movement” has resignified algorithms, taking away the notion of the computational models. He argues that now algorithms are depicted as recipes for carrying out tasks, something for all kinds of information processors, including humans. In contrast to what it used to be: a set of directions to control a computational model, deriving their precision from it. Today, the word “algorithm” is defined as “a step-by-step procedure for solving a problem or accomplishing some end” (Merriam-Webster, 2020). Etymologically, Etymonline (2020) says it comes from a series of transformations that have their origin in al-Khwarizmi the surname of an important mathematician, Muhammad ibn Musa al-Khwarizmi. Computer scientists assumed the challenge to make mindless beings perform complex tasks and it shaped the way they would envision, define and order instructions. Probably the most distinctive force that shapes our minds in a certain way in CS is objectivity at its finest. In a quest to eliminate intuition, subjectivity and ambiguity so those who have none of that (machines) could follow our orders, instructions and data must be so precisely mapped into their meanings that a mechanical, mindless being can “understand” them. That is exactly the special touch algorithms have, they are a special case of a composite instruction, an unambiguous one, which pervades all CS. Additionally, multiple cores and huge distributed systems granted a special place for parallelism on our instructions.|0
Automation|Automation: COMPUTATIONAL THINKING (CT) literature addresses automation as making computers do the work for us (Bocconi et al., 2016; Lee et al., 2011) or using computers as tools (Barr and Stephenson, 2011; Yadav et al., 2014). Sometimes, they are more specific, spotting that the work left for the computers are repetitive or tedious tasks (ISTE and CSTA, 2011). For Lee et al. (2011), “Automation is a labor saving process in which a computer is instructed to execute a set of repetitive tasks quickly and efficiently compared to the processing power of a human”. Worth mention, some papers directly mention (Voogt et al., 2015; Bocconi et al., 2016; Lee et al., 2011) that the creation of artifacts, software (as games and apps) and hardware (as robots and small electronic projects), is part of COMPUTATIONAL THINKING (CT) . Automation is popular as the replacement of human workforce for machinery. Merriam-Webster (2020) defines it as “the technique of making an apparatus, a process, or a system operate automatically.”, while defines automatic as “having a self-acting or self-regulating mechanism.”. Etymologically, Etymonline (2020) says it comes from Greek automatos (“acting of one’s own will”), made of autos (“self”) + matos (“thinking, animated”). Heavily correlated, autonomy comes from Greek autonomia (“independence”), made of autos (“self”) + nomos (“law”). The understanding of the huge gap between human and machine performances in certain tasks is the major distinction of automation in CS. It is about knowing that millions and millions of calculations, that would take us years to complete, are done in a matter of seconds by machines. But also knowing that no matter how much time you give to it, the machine is not going to complete creative tasks, like conceptualizing a good scene for a fantasy illustration. Nowadays we may already imagine feats like this being claimed by deep neural network solutions, but we are not there yet. In the given time, when we get there, we will be entering in a new era of automation.|0
Data|Data: In the COMPUTATIONAL THINKING (CT) literature data often goes along with so called “data practices”, data: gathering/collection, transformation, analysis, representation and visualization (ISTE and CSTA, 2011; Wing, 2011; Barr and Stephenson, 2011; Shute et al., 2017). Data gathering and data collection seems to be used interchangeably, so we are going to keep them as synonyms. Data is also depicted being related to values: “Data involves storing, retrieving, and updating values.” (Brennan and Resnick, 2012). Data representation and transformation usually refer to data structures, as in: “Depicting and organizing data in appropriate graphs, chars, words or images.” (ISTE and CSTA, 2011). Data, as a word, dates back to 1640s, Etymonline (2020) says it is the plural of “datum”, from Latin datum “given”. The meaning evolved through the years, from “a fact given as the basis for calculation in mathematical problems” and “numerical facts collected for future reference” to “transmittable and storable information by which computer operations are performed” (Etymonline, 2020). We cannot say CS created data, but for sure CS warmly adopted and embraced it. Maps and charts are hard to be hand drawn if we are willing to correctly have them in scale. Computer machines have drastically contributed to their plotting, revolutionizing data visualization regardless of the field or context. Those precise representations were already far, if not unreachable from human hands, but as CS advances we use and generate more and more data. It has gone to the point that humans could not handle them without machines automating a massive amount of work, in fact, a whole new field has emerged dedicated entirely to treat, analyze and visualize huge amounts of data using automatic tools and techniques: the Big Data field.|0
Decomposition| Decomposition is often depicted as a pretty straight-forward process of breaking big problems down into smaller, more manageable ones (Barr and Stephenson, 2011; ISTE and CSTA, 2011). Following the same meaning, there are terms like modularizing/modular solutions (Weintrop et al., 2016; Grover and Pea, 2013) and divide-and-conquer (Lu and Fletcher, 2009; Wing, 2011) strategies. Today, the word “decomposition” is defined as “to separate into constituent parts or elements or into simpler compounds” (Merriam-Webster, 2020). Etymologically, Etymonline (2020) says it comes from a prefix de(“the opposite of”) + Latin compositionem, derived from componere (“to collect a whole from several parts”), from com (“together”) + ponere (“to place”). Breaking things down into parts is a matter of (re)organization, it is not limited to a problem solving strategy, but also a natural phenomenon common in the wilds, as seen in biology, or in lab experiments, as seen in chemistry. Due to the limited nature of our computers regarding to memory, power and processing time, CS has a special relation to decomposition because distributing tasks and data across subsystems allow us to treat problems otherwise unbearable. The massive impacts of a good distribution on multicore machines or distributed systems paved the way CS approaches problems.|0
Evaluation|Evaluation and analysis seems to be used interchangeably in the COMPUTATIONAL THINKING (CT) literature, often together, and usually referring to making judgements and/or correctness, as in: “Evaluation is the process of ensuring that a solution, whether an algorithm, system, or process, is a good one: that it is fit for purpose. Various properties of solutions need to be evaluated.” (Csizmadia et al., 2015); “Analysis is a reflective practice that refers to the validation of whether the abstractions made were correct.” (Lee et al., 2011); and “to make trade-offs, by evaluating the use of time and space, power and storage.” (Selby and Woollard, 2014). Merriam-Webster (2020) brings the following definitions for evaluation: “determination of the value, nature, character, or quality of something or someone” and analysis: “a detailed examination of anything complex in order to understand its nature or to determine its essential features: a thorough study”. Etymologically, Etymonline (2020) says evaluation comes from French évaluer (“to find the value of”) and analysis comes from Greek analysis (“a breaking up, a loosening, releasing”), which was used in sentences like “solution of a problem by analysis”. Evaluation has been automated by CS for a long time now. It spread an humongous amount of tools to improve the gathering, storing, sharing and processing of information, which increased accuracy and precision of the evaluation process of researchers across nearly all fields. We can also highlight that we are probably near a next step. Because of CS, we are getting in touch with completely new ways of evaluating, artificial intelligence has brought surprise even to their creators by solving problems in ways we could not anticipate Lehman, Clune and Misevic (2018).|0